# -*- coding: utf-8 -*-
"""Snippets: Importing libraries

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/notebooks/snippets/importing_libraries.ipynb
"""

import numpy as np

def get_bit(bit_array, n):
    """Checks if n-th bit is set (composite)."""
    return (bit_array[n // 8] >> (n % 8)) & 1

def set_bit(bit_array, n):
    """Sets the n-th bit (marks as composite)."""
    bit_array[n // 8] |= (1 << (n % 8))

import numpy as np
from multiprocessing import Pool
import math

def sieve_segment(args):
    start, end, small_primes = args
    size = end - start
    # Bit-packed array for this segment
    seg_bits = np.zeros((size // 8) + 1, dtype=np.uint8)

    for p in small_primes:
        # Find first multiple of p in [start, end]
        first = (start + p - 1) // p * p
        if first < p * p: first = p * p

        for m in range(first, end, p):
            idx = m - start
            seg_bits[idx // 8] |= (1 << (idx % 8))
    return seg_bits

def run_parallel_cpu(N, num_processes=4):
    sqrt_n = int(math.sqrt(N))
    # Pre-compute small primes (Sequential)
    is_prime = np.ones(sqrt_n + 1, dtype=bool)
    for p in range(2, int(sqrt_n**0.5) + 1):
        if is_prime[p]: is_prime[p*p : sqrt_n+1 : p] = False
    small_primes = np.where(is_prime)[0][2:]

    # Partitioning (PCAM: Domain Decomposition)
    seg_size = (N // num_processes)
    tasks = []
    for i in range(num_processes):
        start = i * seg_size + 2
        end = min((i + 1) * seg_size + 2, N + 1)
        tasks.append((start, end, small_primes))

    with Pool(num_processes) as p:
        results = p.map(sieve_segment, tasks)
    return results

import numpy as np
from numba import cuda, uint8, uint32

@cuda.jit
def sieve_kernel(start_val, small_primes, bit_array):
    # Mapping: Each thread processes one small prime
    tid = cuda.grid(1)
    if tid < small_primes.size:
        p = small_primes[tid]
        seg_size = len(bit_array) * 32 # Now each element of bit_array holds 32 bits

        first = (start_val + p - 1) // p * p
        if first < p * p: first = p * p

        for m in range(first, start_val + seg_size, p):
            local_idx = m - start_val
            uint32_idx = uint32(local_idx // 32) # Index into the uint32 array
            bit_in_uint32_idx = local_idx % 32 # Bit position within the uint32 element
            mask = uint32(1 << bit_in_uint32_idx) # Mask should be uint32

            # Agglomeration: Atomic OR
            cuda.atomic.or_(bit_array, uint32_idx, mask)

def run_gpu_sieve(N, segment_size=10**7):
    # 1. Get small primes on CPU
    sqrt_n = int(N**0.5)
    is_prime = np.ones(sqrt_n + 1, dtype=bool)
    for p in range(2, int(sqrt_n**0.5) + 1):
        if is_prime[p]: is_prime[p*p : sqrt_n+1 : p] = False
    small_primes = np.where(is_prime)[0][2:].astype(np.uint32)

    d_small_primes = cuda.to_device(small_primes)
    num_uint32_elements = (segment_size // 32) + 1 # Calculate number of uint32 elements

    # Process in chunks (Segmentation)
    for start in range(2, N + 1, segment_size):
        d_bit_array = cuda.to_device(np.zeros(num_uint32_elements, dtype=np.uint32)) # Initialize with uint32

        threads = 256
        blocks = (len(small_primes) + threads - 1) // threads
        sieve_kernel[blocks, threads](start, d_small_primes, d_bit_array)

        # Results can be copied back or processed on GPU for Sophie Germain primes
        # result = d_bit_array.copy_to_host()

import time
import numpy as np
import matplotlib.pyplot as plt

def benchmark():
    N_values = [10**6, 10**7, 10**8, 10**9]
    cpu_times = []
    gpu_times = []

    for N in N_values:
        print(f"Benchmarking N = {N}...")

        # CPU Benchmark
        start = time.perf_counter()
        run_parallel_cpu(N, num_processes=8)
        cpu_times.append(time.perf_counter() - start)

        # GPU Benchmark
        start = time.perf_counter()
        run_gpu_sieve(N)
        gpu_times.append(time.perf_counter() - start)

    # Plotting results
    plt.figure(figsize=(10, 6))
    plt.loglog(N_values, cpu_times, 'o-', label='OpenMP (8 Cores)')
    plt.loglog(N_values, gpu_times, 's-', label='CUDA GPU')
    plt.title('Scaling Analysis: CPU vs GPU')
    plt.xlabel('N (Range Size)')
    plt.ylabel('Time (seconds)')
    plt.legend()
    plt.grid(True, which="both", ls="-", alpha=0.5)
    plt.savefig('benchmarks/scaling_plot.png')
    print("Benchmark complete. Plot saved to benchmarks/scaling_plot.png")

if __name__ == "__main__":
    benchmark()