{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IqnYOpkButEO"
   },
   "source": [
    "##A Parallel Algorithm for k-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xs6kpaOXz-SG"
   },
   "source": [
    "**Couse Name**: CISC 719 - Contemp Compute Syst Modeling |\n",
    "**Student Name**: Vivi Liao |\n",
    "**Date** : 02/07/2026"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYi3r-5LvSlO"
   },
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aEZjaaDVvT8D"
   },
   "source": [
    "In many modern applications, we must process large, high-dimensional data sets to extract structure and patterns.\n",
    "Clustering is a fundamental unsupervised learning problem that aims to group similar data points together.\n",
    "The *k-means clustering* algorithm is one of the most widely used clustering methods in machine learning and data\n",
    "analysis due to its conceptual simplicity and practical effectiveness.\n",
    "\n",
    "However, the standard (serial) k-means algorithm, also known as Lloyd's algorithm, can become computationally\n",
    "expensive for large data sets: each iteration requires computing distances from every point to every cluster centroid.\n",
    "As data sets grow large and computing platforms become increasingly parallel (\"all computers are now parallel\"),\n",
    "it is essential to design **parallel** algorithms that exploit concurrency from the ground up rather than retrofitting\n",
    "parallelism onto serial designs.\n",
    "\n",
    "This notebook presents the design and analysis of a **parallel k-means clustering algorithm**. The goal is not merely\n",
    "to parallelize an existing implementation, but to systematically apply core principles of parallel algorithm design:\n",
    "\n",
    "- **Recognizing concurrency patterns (Finding Concurrency):** analyzing the problem domain to identify natural\n",
    "  sources of parallelism without thinking about threads or low-level details.\n",
    "- **Structured design spaces and decomposition:** using task and data decomposition to define an architecture-independent\n",
    "  algorithm structure.\n",
    "- **\"Think parallel\" mindset and parallel patterns:** expressing the solution in terms of high-level patterns\n",
    "  (e.g., map, reduce, iterative refinement) while avoiding unnecessary serialization and \"serial traps\".\n",
    "- **Algorithmic theory and validation:** establishing correctness and analyzing parallel time complexity, speedup,\n",
    "  and practical performance, combining formal reasoning with empirical validation.\n",
    "\n",
    "The remainder of this notebook is organized as follows:\n",
    "\n",
    "1. Problem definition and serial baseline for k-means.\n",
    "2. Identification of exploitable concurrency in the problem domain.\n",
    "3. Structured parallel design via data and task decomposition.\n",
    "4. Pattern-based parallel reformulation (\"map\" + \"reduce\" + iterative refinement).\n",
    "5. Correctness argument for the parallel algorithm.\n",
    "6. Work–span analysis, speedup, and discussion of overheads and bottlenecks.\n",
    "7. A small prototype implementation and empirical validation on synthetic data.\n",
    "\n",
    "Throughout, we follow the design philosophy advocated by Quinn, Mattson et al., and McCool et al.:\n",
    "**start from the problem domain, design with concurrency in mind, then reason analytically about correctness\n",
    "and performance before (or alongside) implementation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ihDxanF9vZp6"
   },
   "source": [
    "## 2. Problem Definition and Serial Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ZK4q5lfvemH"
   },
   "source": [
    "### 2.1 k-Means Clustering Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ru6p6MSYvg2a"
   },
   "source": [
    "Let \\(X = \\{x_1, x_2, \\dots, x_n\\}\\) be a set of \\(n\\) data points in \\(\\mathbb{R}^d\\), and let \\(k \\in \\mathbb{N}\\)\n",
    "be the desired number of clusters.\n",
    "\n",
    "The k-means problem seeks to find:\n",
    "\n",
    "- A set of cluster centroids \\(\\{\\mu_1, \\mu_2, \\dots, \\mu_k\\} \\subset \\mathbb{R}^d\\), and\n",
    "- An assignment function \\(\\text{cluster} : \\{1,\\dots,n\\} \\to \\{1,\\dots,k\\}\\)\n",
    "\n",
    "that minimizes the **within-cluster sum of squared distances**:\n",
    "\n",
    "\\[\n",
    "J(\\mu_1, \\dots, \\mu_k, \\text{cluster}) \\;=\\;\n",
    "\\sum_{i=1}^n \\big\\| x_i - \\mu_{\\text{cluster}(i)} \\big\\|^2.\n",
    "\\]\n",
    "\n",
    "Directly solving this optimization problem is NP-hard in general, but the standard iterative procedure\n",
    "known as **Lloyd's algorithm** converges to a local minimum and is widely used in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7E3R_H3vlCy"
   },
   "source": [
    "### 2.2 Serial k-Means (Lloyd's Algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IYEeQ3n4voOu"
   },
   "source": [
    "The serial algorithm proceeds as follows:\n",
    "\n",
    "1. **Initialization:** Choose initial centroids \\(\\mu_1^{(0)}, \\dots, \\mu_k^{(0)}\\) (e.g., randomly or via k-means++).\n",
    "\n",
    "2. **Repeat for iterations \\(t = 0, 1, 2, \\dots\\) until convergence:**\n",
    "\n",
    "   - **Assignment step:**\n",
    "     For each point \\(x_i\\), assign it to the nearest centroid according to Euclidean distance:\n",
    "     \\[\n",
    "     \\text{cluster}^{(t)}(i)\n",
    "     \\;=\\;\n",
    "     \\arg\\min_{j \\in \\{1,\\dots,k\\}} \\|x_i - \\mu_j^{(t)}\\|^2.\n",
    "     \\]\n",
    "\n",
    "   - **Update step:**\n",
    "     For each cluster \\(j \\in \\{1,\\dots,k\\}\\), recompute the centroid as the mean of its assigned points:\n",
    "     \\[\n",
    "     \\mu_j^{(t+1)} =\n",
    "     \\frac{1}{|\\{ i : \\text{cluster}^{(t)}(i) = j \\}|}\n",
    "     \\sum_{i : \\text{cluster}^{(t)}(i) = j} x_i,\n",
    "     \\]\n",
    "     provided the cluster is non-empty. In the rare case of an empty cluster, one common strategy is to\n",
    "     reinitialize the centroid randomly.\n",
    "\n",
    "   - **Convergence check:**\n",
    "     Stop when the centroids stabilize, e.g. when\n",
    "     \\[\n",
    "     \\Delta^{(t)} = \\sum_{j=1}^k \\|\\mu_j^{(t+1)} - \\mu_j^{(t)}\\|^2 < \\varepsilon\n",
    "     \\]\n",
    "     for a small threshold \\(\\varepsilon > 0\\)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "awHCt7t5vr9v"
   },
   "source": [
    "The serial algorithm proceeds as follows:\n",
    "\n",
    "1. **Initialization:** Choose initial centroids \\(\\mu_1^{(0)}, \\dots, \\mu_k^{(0)}\\) (e.g., randomly or via k-means++).\n",
    "\n",
    "2. **Repeat for iterations \\(t = 0, 1, 2, \\dots\\) until convergence:**\n",
    "\n",
    "   - **Assignment step:**\n",
    "     For each point \\(x_i\\), assign it to the nearest centroid according to Euclidean distance:\n",
    "     \\[\n",
    "     \\text{cluster}^{(t)}(i)\n",
    "     \\;=\\;\n",
    "     \\arg\\min_{j \\in \\{1,\\dots,k\\}} \\|x_i - \\mu_j^{(t)}\\|^2.\n",
    "     \\]\n",
    "\n",
    "   - **Update step:**\n",
    "     For each cluster \\(j \\in \\{1,\\dots,k\\}\\), recompute the centroid as the mean of its assigned points:\n",
    "     \\[\n",
    "     \\mu_j^{(t+1)} =\n",
    "     \\frac{1}{|\\{ i : \\text{cluster}^{(t)}(i) = j \\}|}\n",
    "     \\sum_{i : \\text{cluster}^{(t)}(i) = j} x_i,\n",
    "     \\]\n",
    "     provided the cluster is non-empty. In the rare case of an empty cluster, one common strategy is to\n",
    "     reinitialize the centroid randomly.\n",
    "\n",
    "   - **Convergence check:**\n",
    "     Stop when the centroids stabilize, e.g. when\n",
    "     \\[\n",
    "     \\Delta^{(t)} = \\sum_{j=1}^k \\|\\mu_j^{(t+1)} - \\mu_j^{(t)}\\|^2 < \\varepsilon\n",
    "     \\]\n",
    "     for a small threshold \\(\\varepsilon > 0\\)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cwBadXbOv0oo"
   },
   "source": [
    "### 2.3 Serial Time Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BtdW0tjhv3I6"
   },
   "source": [
    "We briefly analyze the time complexity of the serial algorithm as a baseline:\n",
    "\n",
    "- **Assignment step:** For each of the \\(n\\) points, we compute distances to all \\(k\\) centroids, each in \\(O(d)\\) time.\n",
    "  Thus, this step costs \\(O(n k d)\\) per iteration.\n",
    "- **Update step:** For each cluster, we sum all assigned points and divide by the count. Summation over all points\n",
    "  costs \\(O(n d)\\), and final normalization over \\(k\\) clusters costs \\(O(k d)\\), so overall still \\(O(n d)\\) per iteration.\n",
    "\n",
    "The assignment step dominates for \\(k \\geq 2\\), hence the total serial work per iteration is\n",
    "\\[\n",
    "T_{\\text{serial}}^{\\text{iter}} = O(n k d).\n",
    "\\]\n",
    "\n",
    "If the algorithm converges in \\(T\\) iterations, the total serial time is\n",
    "\\[\n",
    "T_{\\text{serial}} = O(T n k d).\n",
    "\\]\n",
    "\n",
    "This complexity motivates parallelization: when \\(n\\) is large and \\(k,d\\) are moderate, the assignment step\n",
    "represents a large amount of **data-parallel work** that can, in principle, be executed concurrently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6a-0Qt5rv6OZ"
   },
   "source": [
    "## 3. Recognizing Concurrency Patterns (Finding Concurrency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bi3SiGSyv8Q3"
   },
   "source": [
    "Following the \"Finding Concurrency\" stage in Mattson et al.'s pattern-oriented design methodology, we now analyze\n",
    "the k-means algorithm at the problem level to identify exploitable parallelism. At this stage we **do not** commit\n",
    "to threads, GPUs, or specific libraries; instead, we identify which computations are independent and can proceed\n",
    "concurrently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vTvnyIFdv-rs"
   },
   "source": [
    "### 3.1 Concurrency in the Assignment Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nuj3-CxVwBMX"
   },
   "source": [
    "For a fixed set of centroids \\(\\{\\mu_j^{(t)}\\}_{j=1}^k\\) at iteration \\(t\\), the assignment for each point\n",
    "\\(x_i\\) is independent:\n",
    "\n",
    "- To compute \\(\\text{cluster}^{(t)}(i)\\), we only need \\(x_i\\) and the current centroids.\n",
    "- There are **no dependencies** between different points' assignments in the same iteration.\n",
    "\n",
    "Thus, we can identify a set of independent logical tasks:\n",
    "\n",
    "- \\(A_i:\\) given point \\(x_i\\) and centroids \\(\\{\\mu_j^{(t)}\\}\\), compute the nearest centroid index.\n",
    "\n",
    "All tasks \\(\\{A_i\\}_{i=1}^n\\) can, in principle, execute concurrently, revealing a **flat data-parallel** structure\n",
    "over the points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6omCL4TwESu"
   },
   "source": [
    "### 3.2 Concurrency in the Update Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IbbLvISPwG_S"
   },
   "source": [
    "Once point assignments \\(\\text{cluster}^{(t)}(i)\\) are known, the new centroid for each cluster is the mean of its\n",
    "assigned points. There are two levels of concurrency here:\n",
    "\n",
    "1. **Across clusters:** Recomputing each centroid \\(\\mu_j^{(t+1)}\\) is independent of the others. Consequently,\n",
    "   we may define tasks\n",
    "   \\[\n",
    "   U_j:\\ \\text{compute the mean of all points assigned to cluster } j,\n",
    "   \\]\n",
    "   and execute \\(\\{U_j\\}_{j=1}^k\\) concurrently.\n",
    "\n",
    "2. **Within each cluster:** The mean is a sum (reduction) over all points in that cluster, followed by division by\n",
    "   the count. Since addition is associative and commutative, the sum can be computed using parallel reduction over\n",
    "   the cluster's assigned points.\n",
    "\n",
    "Therefore, the update step also exhibits data-parallel structure, here naturally viewed as a **reduce by key**\n",
    "pattern: each point contributes to a per-cluster aggregate, and aggregates are combined to form the new centroids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0fDDQKlwPuT"
   },
   "source": [
    "### 3.3 Iterative Structure and Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8nZ7hNqnwQsD"
   },
   "source": [
    "The outer loop over iterations exhibits a **loop-carried dependency**:\n",
    "\n",
    "- The centroids at iteration \\(t+1\\) depend on the centroids at iteration \\(t\\), because assignments are based on\n",
    "  \\(\\{\\mu_j^{(t)}\\}\\).\n",
    "\n",
    "Thus, the algorithm's iterations cannot be fully parallelized in a naive synchronous formulation. However:\n",
    "\n",
    "- Within **each iteration**, the computations over points and clusters are highly parallel.\n",
    "- The critical path of the algorithm consists of one sequence of iterations, each of which involves parallel\n",
    "  assignment and update steps.\n",
    "\n",
    "In summary, the exploitable concurrency is:\n",
    "\n",
    "- Fine-grained parallelism over points in the assignment step.\n",
    "- Coarser parallelism over clusters, and reductions over points, in the update step.\n",
    "- Limited parallelism across iterations (unless asynchronous or speculative variants are considered)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTVGOtYWwTs_"
   },
   "source": [
    "## 4. Structured Design Spaces and Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lr0gIyl4wV-V"
   },
   "source": [
    "Having identified where concurrency exists, we now define a high-level parallel algorithm structure using\n",
    "principled decomposition. In Mattson et al.'s terminology, this corresponds to the \"Algorithm Structure\"\n",
    "design space: we must decide what data and tasks are assigned to which logical processing elements, and how\n",
    "they interact.\n",
    "\n",
    "We consider both **data decomposition** and **task decomposition**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gltcSTaiwYbn"
   },
   "source": [
    "### 4.1 Data Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYX1L3F2wa9I"
   },
   "source": [
    "The most natural decomposition is to partition the data set \\(X\\) into disjoint subsets and assign each subset\n",
    "to a worker. Let \\(P\\) be the number of logical workers (processors, threads, or processes). We partition:\n",
    "\n",
    "\\[\n",
    "X = X_1 \\cup X_2 \\cup \\dots \\cup X_P,\n",
    "\\]\n",
    "where \\(X_p\\) is the subset of points assigned to worker \\(p\\), and the subsets are disjoint.\n",
    "\n",
    "For each worker \\(p \\in \\{1,\\dots,P\\}\\), we define a local task:\n",
    "\n",
    "- **Task \\(T_p\\):** For all points in \\(X_p\\), given the current centroids \\(\\{\\mu_j^{(t)}\\}\\):\n",
    "  1. Compute each point's cluster assignment.\n",
    "  2. Accumulate local per-cluster sums and counts:\n",
    "     \\[\n",
    "     S_{p,j}^{(t)} = \\sum_{x_i \\in X_p, \\ \\text{cluster}(i) = j} x_i,\\quad\n",
    "     C_{p,j}^{(t)} = \\bigl|\\{ x_i \\in X_p : \\text{cluster}(i) = j \\}\\bigr|.\n",
    "     \\]\n",
    "\n",
    "This \"owner-computes\" data decomposition has several advantages:\n",
    "\n",
    "- Each worker reads all centroids but **only writes to its own local accumulators**, reducing contention.\n",
    "- The communication between workers is limited to a small set of aggregated values (per-cluster sums and counts)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "msiWw13rweHr"
   },
   "source": [
    "### 4.2 Task Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hr5fg0XNwgQe"
   },
   "source": [
    "Complementary to the data view, we can enumerate the main task types:\n",
    "\n",
    "- **Assignment tasks:** Independent per-point tasks computing nearest centroid.\n",
    "- **Local reduction tasks:** Per-(worker, cluster) tasks to maintain partial sums and counts.\n",
    "- **Global reduction tasks:** Per-cluster tasks that combine partial sums and counts across workers and compute\n",
    "  new centroids.\n",
    "\n",
    "These tasks can be structured using common patterns:\n",
    "\n",
    "- *Group Tasks:* Combine many fine-grained per-point tasks into a per-worker task \\(T_p\\) to reduce scheduling\n",
    "  overhead.\n",
    "- *Order Tasks:* Within an iteration, enforce the order: centroids \\(\\rightarrow\\) assignment \\(\\rightarrow\\) reduction\n",
    "  \\(\\rightarrow\\) centroid update. Across iterations, enforce the loop-carried dependency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AXce6dEIwiiG"
   },
   "source": [
    "### 4.3 Dependencies and Communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-4gBSd_wk3U"
   },
   "source": [
    "The dependencies can be summarized as follows:\n",
    "\n",
    "- **Within an iteration \\(t\\):**\n",
    "  - All tasks \\(T_p\\) depend on the current centroids \\(\\{\\mu_j^{(t)}\\}\\).\n",
    "  - Each global reduction task for cluster \\(j\\) depends on the local partial results \\(\\{S_{p,j}^{(t)}, C_{p,j}^{(t)}\\}_{p=1}^P\\).\n",
    "  - The convergence check depends on both old and new centroids.\n",
    "\n",
    "- **Across iterations:**\n",
    "  - The assignment tasks in iteration \\(t+1\\) depend on the updated centroids from iteration \\(t\\).\n",
    "\n",
    "Communication pattern:\n",
    "\n",
    "- Broadcast of current centroids to all workers (or shared read in shared memory).\n",
    "- Reduction (sum and count) of per-cluster statistics across workers.\n",
    "\n",
    "This yields an architecture-independent structure that can be mapped onto shared-memory (threads) or distributed\n",
    "(MPI, Spark) implementations with similar high-level behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "isXQ_BK5wnan"
   },
   "source": [
    "## 5. \"Think Parallel\" Mindset and Parallel Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2q_GjgYww-2"
   },
   "source": [
    "We now restate the algorithm using high-level parallel patterns, as advocated by McCool et al.'s \"Thinking Parallel\"\n",
    "philosophy. Instead of focusing on low-level thread management or locks, we formulate the algorithm in terms of\n",
    "**map**, **reduce**, and **iterative refinement** patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bWBRKCsWw0HA"
   },
   "source": [
    "### 5.1 Parallel Patterns Used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64e9pdRww2L4"
   },
   "source": [
    "The parallel k-means algorithm can be expressed as:\n",
    "\n",
    "- **Map:** Apply a function \\(f(x_i)\\) that computes the nearest centroid index for each point \\(x_i\\), independently.\n",
    "- **Reduce by key:** Aggregate contributions from all points into per-cluster sums and counts, using addition\n",
    "  (associative and commutative).\n",
    "- **Iterative refinement:** Repeat the map + reduce cycle using updated centroids until convergence.\n",
    "- **Fork–join:** At each iteration, fork into parallel assignment and local reductions, then join for the global\n",
    "  centroid update and convergence check.\n",
    "\n",
    "In a shared-memory setting, this can be implemented with:\n",
    "\n",
    "- A `parallel_for` (or equivalent) over data points in the assignment step.\n",
    "- Thread-local accumulators to avoid contention, followed by a small reduction over threads.\n",
    "- A parallel loop over clusters to update centroids and compute the convergence measure.\n",
    "\n",
    "In a distributed setting, it maps naturally to:\n",
    "\n",
    "- A data-parallel partition of points over processes.\n",
    "- A collective communication pattern such as `Allreduce` to aggregate per-cluster sums and counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OI587rFyxJrK"
   },
   "source": [
    "### 5.2 Avoiding Serial Traps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gB8l6bbxL4H"
   },
   "source": [
    "A crucial aspect of \"thinking parallel\" is to avoid unnecessary serialization. Some common pitfalls and our design\n",
    "choices are:\n",
    "\n",
    "- **Trap:** Updating global per-cluster sums and counts directly from all threads using locks or atomics.\n",
    "  - This introduces contention and serializes updates.\n",
    "  - **Our choice:** Use **per-thread local accumulators** for \\(\\{S_{p,j}, C_{p,j}\\}\\) and a structured reduction at\n",
    "    the end of the iteration. Only the final combination phase touches shared global data, and that phase operates over\n",
    "    \\(O(k)\\) aggregates rather than \\(O(n)\\) points.\n",
    "\n",
    "- **Trap:** Updating centroids after each individual point assignment.\n",
    "  - This makes assignments sequential, as each update changes the centroids for subsequent points.\n",
    "  - **Our choice:** Treat centroids as read-only within an iteration. We perform a bulk update only after all point\n",
    "    assignments and reductions are complete, preserving independence across points.\n",
    "\n",
    "- **Trap:** Serial convergence checking by scanning all centroids.\n",
    "  - **Our choice:** Compute the convergence measure (e.g., squared centroid movement) in parallel over clusters\n",
    "    using another reduction.\n",
    "\n",
    "These choices ensure that as much of the work as possible is expressed in terms of bulk parallel operations, with\n",
    "only minimal serialization along the algorithm's inherent critical path (iteration order)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LpCGo85BxPYr"
   },
   "source": [
    "## 6. Parallel k-Means Algorithm: Pseudocode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oMEjx8xSxRhB"
   },
   "source": [
    "We now present the parallel algorithm more formally. This description is **architecture-independent**; it can be\n",
    "mapped to different parallel programming models (OpenMP, TBB, MPI, GPUs) while preserving the same logical structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UdUwVav4xT_B"
   },
   "source": [
    "### 6.1 High-Level Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRHyBhh_xbZg"
   },
   "source": [
    "Let \\(X = \\{x_1, \\dots, x_n\\}\\) be the set of data points, and let \\(P\\) be the number of logical workers.\n",
    "\n",
    "1. **Initialization:**\n",
    "   - Choose initial centroids \\(\\{\\mu_j^{(0)}\\}_{j=1}^k\\).\n",
    "   - Partition the data set \\(X\\) into \\(P\\) disjoint subsets \\(X_1, \\dots, X_P\\).\n",
    "\n",
    "2. **For iterations \\(t = 0, 1, 2, \\dots\\) until convergence:**\n",
    "\n",
    "   **(a) Parallel assignment and local accumulation (map + local reduce):**\n",
    "\n",
    "   For each worker \\(p \\in \\{1,\\dots,P\\}\\) in parallel:\n",
    "\n",
    "   - Initialize local sums and counts:\n",
    "     \\[\n",
    "     S_{p,j}^{(t)} \\leftarrow 0 \\in \\mathbb{R}^d,\\quad\n",
    "     C_{p,j}^{(t)} \\leftarrow 0,\\quad \\forall j \\in \\{1,\\dots,k\\}.\n",
    "     \\]\n",
    "   - For each point \\(x_i \\in X_p\\):\n",
    "     1. Compute nearest centroid:\n",
    "        \\[\n",
    "        j^* = \\arg\\min_{j \\in \\{1,\\dots,k\\}} \\|x_i - \\mu_j^{(t)}\\|^2.\n",
    "        \\]\n",
    "     2. Assign point \\(x_i\\) to cluster \\(j^*\\) (store \\(\\text{cluster}^{(t)}(i)\\)).\n",
    "     3. Update local statistics:\n",
    "        \\[\n",
    "        S_{p,j^*}^{(t)} \\leftarrow S_{p,j^*}^{(t)} + x_i,\\quad\n",
    "        C_{p,j^*}^{(t)} \\leftarrow C_{p,j^*}^{(t)} + 1.\n",
    "        \\]\n",
    "\n",
    "   **(b) Global centroid update (global reduction):**\n",
    "\n",
    "   For each cluster \\(j \\in \\{1,\\dots,k\\}\\) (possibly in parallel over \\(j\\)):\n",
    "\n",
    "   - Combine local partials into global sums and counts:\n",
    "     \\[\n",
    "     S_j^{(t)} = \\sum_{p=1}^P S_{p,j}^{(t)},\\quad\n",
    "     C_j^{(t)} = \\sum_{p=1}^P C_{p,j}^{(t)}.\n",
    "     \\]\n",
    "   - If \\(C_j^{(t)} > 0\\), update centroid:\n",
    "     \\[\n",
    "     \\mu_j^{(t+1)} = \\frac{S_j^{(t)}}{C_j^{(t)}}.\n",
    "     \\]\n",
    "     Otherwise, reinitialize \\(\\mu_j^{(t+1)}\\) (e.g. to a random point).\n",
    "\n",
    "   **(c) Convergence check (parallel reduction):**\n",
    "\n",
    "   Compute the change in centroids:\n",
    "   \\[\n",
    "   \\Delta^{(t)} = \\sum_{j=1}^k \\|\\mu_j^{(t+1)} - \\mu_j^{(t)}\\|^2.\n",
    "   \\]\n",
    "   If \\(\\Delta^{(t)} < \\varepsilon\\), terminate.\n",
    "\n",
    "This algorithm uses data decomposition over points, combined with task decomposition into assignment, local reduction,\n",
    "and global reduction tasks. The design is captured by well-known parallel patterns and avoids unnecessary synchronization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nfxgbSJtxWFV"
   },
   "source": [
    "## 7. Correctness of the Parallel Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-xXH-TiwqrE"
   },
   "source": [
    "We now argue that the parallel k-means algorithm is correct in the sense that:\n",
    "\n",
    "1. For a fixed initialization and deterministic tie-breaking, the parallel algorithm produces the same sequence of\n",
    "   centroid sets \\(\\{\\mu_j^{(t)}\\}\\) as the serial Lloyd's algorithm.\n",
    "2. Therefore, all standard convergence properties of Lloyd's algorithm carry over."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UFGohyz8xoh1"
   },
   "source": [
    "### 7.1 Functional Equivalence to Serial Lloyd's Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHNU0WEjxy1s"
   },
   "source": [
    "We consider one iteration \\(t\\) and show that the results of the parallel variant are identical to those of the serial\n",
    "algorithm (up to floating-point rounding).\n",
    "\n",
    "1. **Assignment step:**\n",
    "   - In both the serial and parallel algorithms, the centroids used for assignment are the same set\n",
    "     \\(\\{\\mu_j^{(t)}\\}_{j=1}^k\\), and they remain fixed throughout the assignment phase.\n",
    "   - For each point \\(x_i\\), both algorithms compute\n",
    "     \\[\n",
    "     \\text{cluster}^{(t)}(i) = \\arg\\min_{j} \\|x_i - \\mu_j^{(t)}\\|^2.\n",
    "     \\]\n",
    "   - The parallel algorithm processes points in an arbitrary order across workers, but since each point's assignment\n",
    "     depends only on its own distances to the fixed centroids, the resulting cluster assignments for all \\(i\\) are\n",
    "     identical to those of the serial algorithm.\n",
    "\n",
    "2. **Update step:**\n",
    "   - Given identical assignments \\(\\{\\text{cluster}^{(t)}(i)\\}\\), both algorithms compute the new centroids as\n",
    "     means of cluster members.\n",
    "   - The parallel algorithm computes\n",
    "     \\[\n",
    "     S_{p,j}^{(t)} = \\sum_{x_i \\in X_p, \\ \\text{cluster}(i) = j} x_i,\\quad\n",
    "     C_{p,j}^{(t)} = \\bigl|\\{x_i \\in X_p : \\text{cluster}(i) = j\\}\\bigr|,\n",
    "     \\]\n",
    "     and then\n",
    "     \\[\n",
    "     S_j^{(t)} = \\sum_{p=1}^P S_{p,j}^{(t)} = \\sum_{i: \\text{cluster}(i)=j} x_i,\\quad\n",
    "     C_j^{(t)} = \\sum_{p=1}^P C_{p,j}^{(t)} = \\bigl|\\{i : \\text{cluster}(i) = j\\}\\bigr|.\n",
    "     \\]\n",
    "   - The final centroids are\n",
    "     \\[\n",
    "     \\mu_j^{(t+1)} = \\frac{S_j^{(t)}}{C_j^{(t)}},\n",
    "     \\]\n",
    "     which are exactly the same as in the serial algorithm. Order of summation does not affect the result in exact\n",
    "     arithmetic (and in floating-point arithmetic differences are only due to rounding).\n",
    "\n",
    "Therefore, each iteration of the parallel algorithm produces the same updated centroids as the serial algorithm.\n",
    "By induction over iterations \\(t\\), the sequence \\(\\{\\mu_j^{(t)}\\}\\) is identical for both algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZPOxy9Ex0OQ"
   },
   "source": [
    "### 7.2 Convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bkZTHeW9x4PR"
   },
   "source": [
    "It is well-known that standard Lloyd's k-means algorithm monotonically decreases the objective function\n",
    "\\[\n",
    "J = \\sum_{i=1}^n \\|x_i - \\mu_{\\text{cluster}(i)}\\|^2\n",
    "\\]\n",
    "at each iteration, and converges in a finite number of steps to a local minimum.\n",
    "\n",
    "Since the parallel algorithm is functionally equivalent to Lloyd's algorithm (with the same initialization and\n",
    "tie-breaking), it inherits the same convergence properties.\n",
    "\n",
    "In summary, the parallelization strategy preserves **functional correctness**: it does not change which solution\n",
    "is computed, only how the computation is performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZ19LV2ex6zn"
   },
   "source": [
    "## 8. Parallel Complexity Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2MLNn7qLx9AG"
   },
   "source": [
    "To analyze the performance of the parallel algorithm, we use a standard **work–span** model:\n",
    "\n",
    "- \\(T_1\\): total work (time on a single processor).\n",
    "- \\(T_\\infty\\): span or critical path length (time on an infinite number of processors).\n",
    "- \\(T_P\\): time on \\(P\\) processors, ideally satisfying\n",
    "  \\[\n",
    "  T_P \\approx \\max\\left( \\frac{T_1}{P},\\ T_\\infty \\right).\n",
    "  \\]\n",
    "\n",
    "Let:\n",
    "\n",
    "- \\(n\\) = number of points,\n",
    "- \\(k\\) = number of clusters,\n",
    "- \\(d\\) = dimension,\n",
    "- \\(T\\) = number of iterations until convergence,\n",
    "- \\(P\\) = number of processors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tNcAtwB2yAL4"
   },
   "source": [
    "### 8.1 Work \\(T_1\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PPsTubhiyCg2"
   },
   "source": [
    "The work of the parallel algorithm is asymptotically the same as that of the serial algorithm:\n",
    "\n",
    "- Assignment step per iteration: \\(O(n k d)\\).\n",
    "- Update step per iteration: \\(O(n d)\\) for summations plus \\(O(k d)\\) for final centroid computation.\n",
    "\n",
    "Thus,\n",
    "\\[\n",
    "T_1^{\\text{iter}} = O(n k d), \\quad\n",
    "T_1 = O(T n k d).\n",
    "\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9BSgJ4pOyE-V"
   },
   "source": [
    "### 8.2 Span \\(T_\\infty\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kl7RFMxfyG9L"
   },
   "source": [
    "Assume that:\n",
    "\n",
    "- We have enough processors such that the point-wise computations can proceed in parallel.\n",
    "- Reductions are implemented as tree-based parallel reductions.\n",
    "\n",
    "For a single iteration:\n",
    "\n",
    "1. **Assignment step:**\n",
    "   - Each point \\(x_i\\) requires \\(O(k d)\\) work to compute distances to all centroids.\n",
    "   - In the idealized infinite-processor model, these per-point computations can execute in parallel, so the span\n",
    "     of this phase is \\(O(k d)\\), corresponding to the work for one point.\n",
    "\n",
    "2. **Local accumulation and global reduction:**\n",
    "   - Local accumulation is fused with assignment and thus does not increase the span beyond \\(O(k d)\\).\n",
    "   - Combining local partial results for each cluster can be done using a tree reduction over \\(P\\) workers,\n",
    "     with span \\(O(\\log P)\\). Since reductions for different clusters can be done in parallel, the span is still\n",
    "     \\(O(\\log P)\\).\n",
    "\n",
    "3. **Centroid update and convergence check:**\n",
    "   - Updating \\(k\\) centroids and computing the convergence measure involves \\(O(k d)\\) work and can be parallelized\n",
    "     over clusters. The span of this phase is \\(O(d)\\).\n",
    "\n",
    "Therefore, the span per iteration is:\n",
    "\n",
    "\\[\n",
    "T_\\infty^{\\text{iter}} = O(k d + \\log P).\n",
    "\\]\n",
    "\n",
    "Across iterations, the loop-carried dependence imposes a factor of \\(T\\), so the overall span is:\n",
    "\n",
    "\\[\n",
    "T_\\infty = O\\bigl(T (k d + \\log P)\\bigr).\n",
    "\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XeW5Cfy2yKo1"
   },
   "source": [
    "### 8.3 Parallel Time \\(T_P\\) and Speedup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8OhB8MSgyM_H"
   },
   "source": [
    "On \\(P\\) processors, ignoring constant factors and low-level overheads, we have:\n",
    "\n",
    "\\[\n",
    "T_P \\approx \\max\\left( \\frac{T_1}{P},\\ T_\\infty \\right)\n",
    "= \\max\\left( O\\left(\\frac{T n k d}{P}\\right),\\ O\\bigl(T (k d + \\log P)\\bigr) \\right).\n",
    "\\]\n",
    "\n",
    "For large \\(n\\), the first term dominates, and we expect near-linear speedup until:\n",
    "\n",
    "\\[\n",
    "\\frac{T n k d}{P} \\approx T (k d + \\log P)\n",
    "\\quad \\Rightarrow \\quad\n",
    "n \\approx P \\left( 1 + \\frac{\\log P}{k d} \\right).\n",
    "\\]\n",
    "\n",
    "Roughly, as long as \\(n \\gg P (k d + \\log P)\\), the algorithm is **work-efficient** and achieves good parallel speedup.\n",
    "\n",
    "The ideal speedup is:\n",
    "\n",
    "\\[\n",
    "\\text{Speedup}(P) = \\frac{T_1}{T_P} \\le \\frac{T_1}{T_\\infty}\n",
    "= O\\left(\\frac{n k d}{k d + \\log P}\\right).\n",
    "\\]\n",
    "\n",
    "For fixed \\(k,d\\) and sufficiently large \\(n\\), this approaches \\(O\\!\\left(\\frac{n}{\\log P}\\right)\\), but in practice\n",
    "the dominant factor is how \\(n\\) compares to \\(P\\)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qA9oF4y7yP_Q"
   },
   "source": [
    "### 8.4 Overheads and Bottlenecks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DbVBNj9HySXY"
   },
   "source": [
    "Several practical factors influence real-world performance:\n",
    "\n",
    "- **Synchronization:** Each iteration requires at least one global synchronization point to complete the reduction\n",
    "  of per-cluster sums and counts and update centroids.\n",
    "\n",
    "- **Communication:** In a distributed-memory implementation, reductions over \\(\\{S_{p,j}, C_{p,j}\\}\\) require\n",
    "  `Allreduce` or equivalent collective communication, so communication costs must be accounted for.\n",
    "\n",
    "- **Load balancing:** With a uniform data decomposition (equal numbers of points per worker) and uniform\n",
    "  per-point cost, load balance is good. If computation per point varies (e.g., due to sparsity or varying dimensionality),\n",
    "  dynamic scheduling or work-stealing may be beneficial.\n",
    "\n",
    "- **Memory bandwidth and locality:** The assignment step can be memory-bandwidth-bound if the data and centroids\n",
    "  do not fit into cache. Using contiguous storage for centroids and points and aligning data structures can improve\n",
    "  locality.\n",
    "\n",
    "Overall, the algorithm is amenable to efficient parallelization because:\n",
    "\n",
    "- The majority of work is in embarrassingly parallel per-point computations.\n",
    "- The sequential fraction per iteration (centroid update and convergence check) is small.\n",
    "- Communication volume per iteration is proportional to \\(k d\\), not \\(n\\)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yRVu7EsNyVCD"
   },
   "source": [
    "## 9. Prototype Implementation and Empirical Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W1ERof26yXkH"
   },
   "source": [
    "To complement the theoretical analysis, we now implement a simple prototype of the parallel k-means algorithm\n",
    "in Python and perform small-scale experiments on synthetic data.\n",
    "\n",
    "The goals are:\n",
    "\n",
    "- To validate functional correctness by comparing serial and \"parallel\" (multi-process) implementations.\n",
    "- To observe parallel speedup as the number of processes and data size increase.\n",
    "\n",
    "**Note:** Python is not ideal for high-performance parallel numerics due to the Global Interpreter Lock (GIL) and other overheads. Nevertheless, using `multiprocessing` with separate processes can demonstrate the algorithmic structure and give qualitative insight into scaling. In a production setting, one would likely implement this in C++ with OpenMP/TBB, use MPI, or rely on GPU-accelerated libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kM3Wi_aCyZ2-"
   },
   "source": [
    "### 9.1 Serial Reference Implementation\n",
    "\n",
    "We first implement a straightforward serial k-means algorithm that follows Lloyd's procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "BioqoDEHtrvH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "import time\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 9.1 Serial Reference Implementation\n",
    "#\n",
    "# We first implement a straightforward serial k-means algorithm that follows Lloyd's procedure.\n",
    "\n",
    "# %%\n",
    "def kmeans_serial(X, k, max_iters=100, tol=1e-4, random_state=0):\n",
    "    \"\"\"\n",
    "    Serial Lloyd-style k-means implementation.\n",
    "    X: (n, d) array of data points\n",
    "    k: number of clusters\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    n, d = X.shape\n",
    "\n",
    "    # Initialize centroids by sampling k distinct points\n",
    "    indices = rng.choice(n, size=k, replace=False)\n",
    "    centroids = X[indices].copy()\n",
    "\n",
    "    labels = np.zeros(n, dtype=np.int64)\n",
    "\n",
    "    for it in range(max_iters):\n",
    "        # Assignment step\n",
    "        # Compute squared distances to each centroid\n",
    "        # Shape of distances: (n, k)\n",
    "        diff = X[:, None, :] - centroids[None, :, :]  # (n, k, d)\n",
    "        dists = np.sum(diff ** 2, axis=2)\n",
    "        new_labels = np.argmin(dists, axis=1)\n",
    "\n",
    "        # Update step\n",
    "        new_centroids = np.zeros_like(centroids)\n",
    "        for j in range(k):\n",
    "            mask = (new_labels == j)\n",
    "            if np.any(mask):\n",
    "                new_centroids[j] = X[mask].mean(axis=0)\n",
    "            else:\n",
    "                # Reinitialize empty cluster centroid randomly\n",
    "                new_centroids[j] = X[rng.integers(0, n)]\n",
    "\n",
    "        # Convergence check\n",
    "        shift = np.sum((new_centroids - centroids) ** 2)\n",
    "        centroids = new_centroids\n",
    "        labels = new_labels\n",
    "\n",
    "        if shift < tol:\n",
    "            break\n",
    "\n",
    "    return centroids, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VY8Dtt2yyiRk"
   },
   "source": [
    "### 9.2 Parallel Helper: Per-Chunk Assignment and Local Accumulation\n",
    "\n",
    "We now implement a helper function that performs the assignment and local accumulation for a *chunk* of points.\n",
    "This corresponds directly to the task \\(T_p\\) in our parallel design.\n",
    "\n",
    "The function takes:\n",
    "\n",
    "- `X_chunk`: a subset of points,\n",
    "- `centroids`: current global centroids,\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `local_sums`: an array of shape `(k, d)` with per-cluster sums,\n",
    "- `local_counts`: an array of shape `(k,)` with per-cluster counts,\n",
    "- `local_labels`: the cluster labels for the points in the chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ryswS-rytuXU"
   },
   "outputs": [],
   "source": [
    "def assign_and_accumulate_chunk(X_chunk, centroids):\n",
    "    \"\"\"\n",
    "    Assign points in X_chunk to nearest centroids and compute local sums and counts.\n",
    "    Returns (local_sums, local_counts, local_labels).\n",
    "    \"\"\"\n",
    "    n_chunk, d = X_chunk.shape\n",
    "    k = centroids.shape[0]\n",
    "\n",
    "    # Compute distances\n",
    "    diff = X_chunk[:, None, :] - centroids[None, :, :]\n",
    "    dists = np.sum(diff ** 2, axis=2)\n",
    "    labels_chunk = np.argmin(dists, axis=1)\n",
    "\n",
    "    local_sums = np.zeros_like(centroids)\n",
    "    local_counts = np.zeros(k, dtype=np.int64)\n",
    "\n",
    "    for i in range(n_chunk):\n",
    "        j = labels_chunk[i]\n",
    "        local_sums[j] += X_chunk[i]\n",
    "        local_counts[j] += 1\n",
    "\n",
    "    return local_sums, local_counts, labels_chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ogA8CkjSyoF3"
   },
   "source": [
    "### 9.3 Parallel k-Means with Multiprocessing\n",
    "To simulate a parallel environment, we use Python's `multiprocessing.Pool` to process chunks of the data in\n",
    "separate processes. This is only a simple prototype, but it matches the structure of our algorithm:\n",
    "\n",
    "- Partition `X` into `P` chunks.\n",
    "- At each iteration:\n",
    "  - In parallel, call `assign_and_accumulate_chunk` for each chunk.\n",
    "  - Reduce partial sums and counts to obtain global centroids.\n",
    "  - Check for convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "nLzTEdiFt1Xp"
   },
   "outputs": [],
   "source": [
    "def kmeans_parallel(X, k, n_procs=None, max_iters=100, tol=1e-4, random_state=0):\n",
    "    \"\"\"\n",
    "    Parallel k-means using Python multiprocessing.\n",
    "    X: (n, d) array\n",
    "    k: number of clusters\n",
    "    n_procs: number of worker processes (defaults to CPU count)\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    n, d = X.shape\n",
    "    if n_procs is None:\n",
    "        n_procs = min(cpu_count(), 8)  # cap for demonstration\n",
    "\n",
    "    # Initialize centroids\n",
    "    indices = rng.choice(n, size=k, replace=False)\n",
    "    centroids = X[indices].copy()\n",
    "\n",
    "    # Partition data into chunks\n",
    "    chunks = np.array_split(X, n_procs, axis=0)\n",
    "\n",
    "    labels = np.zeros(n, dtype=np.int64)\n",
    "\n",
    "    with Pool(processes=n_procs) as pool:\n",
    "        for it in range(max_iters):\n",
    "            # Broadcast centroids to workers via partial\n",
    "            func = partial(assign_and_accumulate_chunk, centroids=centroids)\n",
    "\n",
    "            # Map over chunks in parallel\n",
    "            results = pool.map(func, chunks)\n",
    "\n",
    "            # Reduction over local results\n",
    "            global_sums = np.zeros_like(centroids)\n",
    "            global_counts = np.zeros(k, dtype=np.int64)\n",
    "\n",
    "            # Also reconstruct labels (for correctness comparison)\n",
    "            # Note: this assembling step is not fully parallel in this prototype.\n",
    "            offset = 0\n",
    "            for (local_sums, local_counts, labels_chunk) in results:\n",
    "                global_sums += local_sums\n",
    "                global_counts += local_counts\n",
    "                labels[offset:offset+len(labels_chunk)] = labels_chunk\n",
    "                offset += len(labels_chunk)\n",
    "\n",
    "            new_centroids = np.zeros_like(centroids)\n",
    "            for j in range(k):\n",
    "                if global_counts[j] > 0:\n",
    "                    new_centroids[j] = global_sums[j] / global_counts[j]\n",
    "                else:\n",
    "                    new_centroids[j] = X[rng.integers(0, n)]\n",
    "\n",
    "            shift = np.sum((new_centroids - centroids) ** 2)\n",
    "            centroids = new_centroids\n",
    "\n",
    "            if shift < tol:\n",
    "                break\n",
    "\n",
    "    return centroids, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ByN-yyszywJK"
   },
   "source": [
    "## 10. Empirical Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y45A944myw1b"
   },
   "source": [
    "We now perform a small set of experiments on synthetic data to:\n",
    "\n",
    "1. Verify that the serial and parallel implementations produce similar final centroids and cluster assignments.\n",
    "2. Compare runtimes and observe parallel speedup as the number of processes and data size changes.\n",
    "\n",
    "These experiments are not intended as high-precision benchmarks, but rather as a validation of the design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VFC4MHyBuCU6",
    "outputId": "f25a4996-6466-4993-e6fa-c246b187d404"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serial centroids:\n",
      " [[-4.03770903  4.77204021]\n",
      " [ 2.5852474   2.88851467]\n",
      " [ 2.72924521 -0.6299956 ]\n",
      " [ 3.56501422  1.91519303]]\n",
      "Parallel centroids:\n",
      " [[-4.03770903  4.77204021]\n",
      " [ 2.5852474   2.88851467]\n",
      " [ 2.72924521 -0.6299956 ]\n",
      " [ 3.56501422  1.91519303]]\n",
      "Centroid difference (Frobenius norm): 2.4725840765205216e-15\n",
      "n= 10000 | serial= 0.083s | parallel(4 procs)= 0.665s | speedup= 0.13x\n",
      "n= 20000 | serial= 0.149s | parallel(4 procs)= 0.645s | speedup= 0.23x\n",
      "n= 40000 | serial= 0.284s | parallel(4 procs)= 1.231s | speedup= 0.23x\n"
     ]
    }
   ],
   "source": [
    "def generate_gaussian_mixture(n, d, k, spread=0.5, random_state=0):\n",
    "    \"\"\"\n",
    "    Generate synthetic data from a mixture of k Gaussians in R^d.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    centers = rng.uniform(-5, 5, size=(k, d))\n",
    "    points_per_cluster = n // k\n",
    "    X_list = []\n",
    "    for j in range(k):\n",
    "        cov = (spread ** 2) * np.eye(d)\n",
    "        X_j = rng.multivariate_normal(centers[j], cov, size=points_per_cluster)\n",
    "        X_list.append(X_j)\n",
    "    X = np.vstack(X_list)\n",
    "    return X\n",
    "\n",
    "# %%\n",
    "# Small correctness test\n",
    "n, d, k = 2000, 2, 4\n",
    "X_small = generate_gaussian_mixture(n, d, k, random_state=42)\n",
    "\n",
    "cent_serial, lab_serial = kmeans_serial(X_small, k, max_iters=50, tol=1e-6, random_state=1)\n",
    "cent_parallel, lab_parallel = kmeans_parallel(X_small, k, n_procs=4, max_iters=50, tol=1e-6, random_state=1)\n",
    "\n",
    "# Compare centroids (up to permutation; here we simply sort by first coordinate for rough comparison)\n",
    "def sort_centroids(C):\n",
    "    return C[np.argsort(C[:, 0])]\n",
    "\n",
    "cs = sort_centroids(cent_serial)\n",
    "cp = sort_centroids(cent_parallel)\n",
    "\n",
    "print(\"Serial centroids:\\n\", cs)\n",
    "print(\"Parallel centroids:\\n\", cp)\n",
    "print(\"Centroid difference (Frobenius norm):\", np.linalg.norm(cs - cp))\n",
    "\n",
    "# %%\n",
    "# Simple runtime comparison for increasing n and fixed k,d\n",
    "sizes = [10_000, 20_000, 40_000]\n",
    "d, k = 10, 5\n",
    "n_procs = 4\n",
    "\n",
    "results_timing = []\n",
    "\n",
    "for n in sizes:\n",
    "    X = generate_gaussian_mixture(n, d, k, random_state=0)\n",
    "\n",
    "    t0 = time.time()\n",
    "    kmeans_serial(X, k, max_iters=20, tol=1e-4, random_state=0)\n",
    "    t_serial = time.time() - t0\n",
    "\n",
    "    t0 = time.time()\n",
    "    kmeans_parallel(X, k, n_procs=n_procs, max_iters=20, tol=1e-4, random_state=0)\n",
    "    t_parallel = time.time() - t0\n",
    "\n",
    "    results_timing.append((n, t_serial, t_parallel))\n",
    "    print(f\"n={n:6d} | serial={t_serial:6.3f}s | parallel({n_procs} procs)={t_parallel:6.3f}s | speedup={t_serial/t_parallel:5.2f}x\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5Rn_MJvy4-6"
   },
   "source": [
    "The small experiments above should show:\n",
    "\n",
    "- The serial and parallel centroids are very close (small Frobenius norm difference), validating functional equivalence\n",
    "  up to floating-point rounding and cluster permutation.\n",
    "- Parallel execution with `n_procs=4` yields a speedup greater than 1 for sufficiently large `n`, although\n",
    "  Python and multiprocessing overheads limit the magnitude of speedup.\n",
    "\n",
    "In a more optimized implementation (e.g., C++ with OpenMP or MPI), and for much larger `n`, the speedup would\n",
    "more closely match the theoretical analysis: near-linear scaling until the effects of communication, synchronization,\n",
    "and memory bandwidth become dominant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q2C7URiQy8By"
   },
   "source": [
    "## 11. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFpeaQMZy-Yb"
   },
   "source": [
    "In this notebook, we designed and analyzed a parallel algorithm for k-means clustering, explicitly following\n",
    "foundational principles of parallel algorithm design:\n",
    "\n",
    "- We **identified concurrency** in the problem domain by recognizing that, for a fixed set of centroids, point-wise\n",
    "  cluster assignments and per-cluster aggregations can be computed independently.\n",
    "- We used **data decomposition** (partitioning the data set across workers) and **task decomposition**\n",
    "  (assignment, local reduction, global reduction, convergence checking) to define a clear algorithm structure.\n",
    "- We embraced a **\"think parallel\" mindset** by formulating the algorithm in terms of high-level patterns —\n",
    "  map, reduce (by key), and iterative refinement — and by avoiding serial traps such as frequent global updates\n",
    "  or order-dependent operations.\n",
    "- We established **correctness** by showing that, given the same initialization, the parallel algorithm produces\n",
    "  the same sequence of centroid updates as the serial Lloyd's algorithm.\n",
    "- We performed a **work–span analysis** to characterize the parallel complexity and potential speedup, and briefly\n",
    "  discussed key overheads and bottlenecks.\n",
    "- We implemented a small prototype using Python's multiprocessing library to **empirically validate** both\n",
    "  functional equivalence and parallel speedup on synthetic data.\n",
    "\n",
    "This design-centric approach lays a solid foundation for further optimization. Once the algorithm's structure\n",
    "is correctly parallelized and analytically understood, we can apply more advanced techniques such as:\n",
    "\n",
    "- Reducing communication (e.g., mini-batch or asynchronous k-means),\n",
    "- Improving memory locality (e.g., blocking, cache-friendly layouts),\n",
    "- Refining initialization and convergence criteria (e.g., k-means++, early stopping),\n",
    "- Mapping the algorithm efficiently onto specific architectures (GPUs, distributed clusters).\n",
    "\n",
    "Ultimately, the methodology illustrated here — starting from concurrency analysis, using structured decompositions\n",
    "and patterns, and grounding the design in algorithmic theory — is applicable well beyond k-means clustering\n",
    "and can guide the development of scalable parallel algorithms for many other computational problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Youtube Video for the presentation: https://youtu.be/kt-XL6EVr50"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
